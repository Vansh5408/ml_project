{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f25932fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2025.8.29-cp39-cp39-macosx_11_0_arm64.whl (286 kB)\n",
      "\u001b[K     |████████████████████████████████| 286 kB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from nltk) (1.5.2)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2025.8.29 tqdm-4.67.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adityakumar/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords') # for stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe70bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged and started processing the data!\n",
      "   movie_id                                     title  \\\n",
      "0     19995                                    Avatar   \n",
      "1       285  Pirates of the Caribbean: At World's End   \n",
      "2    206647                                   Spectre   \n",
      "3     49026                     The Dark Knight Rises   \n",
      "4     49529                               John Carter   \n",
      "\n",
      "                                            overview  \\\n",
      "0  In the 22nd century, a paraplegic Marine is di...   \n",
      "1  Captain Barbossa, long believed to be dead, ha...   \n",
      "2  A cryptic message from Bond’s past sends him o...   \n",
      "3  Following the death of District Attorney Harve...   \n",
      "4  John Carter is a war-weary, former military ca...   \n",
      "\n",
      "                                     genres  \\\n",
      "0  Action Adventure Fantasy Science Fiction   \n",
      "1                  Adventure Fantasy Action   \n",
      "2                    Action Adventure Crime   \n",
      "3               Action Crime Drama Thriller   \n",
      "4          Action Adventure Science Fiction   \n",
      "\n",
      "                                            keywords  \\\n",
      "0  culture clash future space war space colony so...   \n",
      "1  ocean drug abuse exotic island east india trad...   \n",
      "2  spy based on novel secret agent sequel mi6 bri...   \n",
      "3  dc comics crime fighter terrorist secret ident...   \n",
      "4  based on novel mars medallion space travel pri...   \n",
      "\n",
      "                                           cast  \n",
      "0  Sam Worthington Zoe Saldana Sigourney Weaver  \n",
      "1     Johnny Depp Orlando Bloom Keira Knightley  \n",
      "2      Daniel Craig Christoph Waltz Léa Seydoux  \n",
      "3      Christian Bale Michael Caine Gary Oldman  \n",
      "4    Taylor Kitsch Lynn Collins Samantha Morton  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# --- Step 1: Load BOTH datasets ---\n",
    "movies_df = pd.read_csv('tmdb_5000_movies.csv')\n",
    "credits_df = pd.read_csv('tmdb_5000_credits.csv')\n",
    "\n",
    "# --- Step 2: Merge them into a single DataFrame ---\n",
    "# Note: The first file uses 'id', the second uses 'movie_id'. We merge on these.\n",
    "df = movies_df.merge(credits_df, left_on='id', right_on='movie_id')\n",
    "\n",
    "\n",
    "# --- Step 3: Now, select the columns from the new 'df' ---\n",
    "# Your original code will now work perfectly. We'll also add 'cast' which is very useful.\n",
    "movies = df[['id', 'title_x', 'overview', 'genres', 'keywords', 'cast']].copy()\n",
    "# We use 'title_x' because the merge creates 'title_x' and 'title_y'. They are the same.\n",
    "movies.rename(columns={'id': 'movie_id', 'title_x': 'title'}, inplace=True)\n",
    "\n",
    "\n",
    "# --- Step 4: Your original cleaning code continues from here ---\n",
    "# Function to extract names from JSON-like columns\n",
    "def extract_names(text):\n",
    "    # Check if the text is a valid list of dictionaries\n",
    "    try:\n",
    "        items = json.loads(text)\n",
    "        names = [item['name'] for item in items]\n",
    "        return \" \".join(names)\n",
    "    except (TypeError, json.JSONDecodeError):\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract top 3 cast members\n",
    "def extract_cast(text):\n",
    "    try:\n",
    "        items = json.loads(text)\n",
    "        names = [item['name'] for item in items[:3]] # Get top 3 actors\n",
    "        return \" \".join(names)\n",
    "    except (TypeError, json.JSONDecodeError):\n",
    "        return \"\"\n",
    "\n",
    "# Apply the functions\n",
    "movies['genres'] = movies['genres'].apply(extract_names)\n",
    "movies['keywords'] = movies['keywords'].apply(extract_names)\n",
    "movies['cast'] = movies['cast'].apply(extract_cast)\n",
    "# Handle missing overview data\n",
    "movies['overview'] = movies['overview'].fillna('')\n",
    "\n",
    "print(\"Successfully merged and started processing the data!\")\n",
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76072271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data preprocessing is complete! The final cleaned data has been saved to 'processed_movies.csv'\n",
      "\n",
      "Here's a sample of the final data:\n",
      "   movie_id                                     title  \\\n",
      "0     19995                                    Avatar   \n",
      "1       285  Pirates of the Caribbean: At World's End   \n",
      "2    206647                                   Spectre   \n",
      "3     49026                     The Dark Knight Rises   \n",
      "4     49529                               John Carter   \n",
      "\n",
      "                                        cleaned_tags  \n",
      "0  nd century paraplegic marine dispatched moon p...  \n",
      "1  captain barbossa long believed dead come back ...  \n",
      "2  cryptic message bonds past sends trail uncover...  \n",
      "3  following death district attorney harvey dent ...  \n",
      "4  john carter warweary former military captain w...  \n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Combine all text columns into a single 'tags' column ---\n",
    "# We are also including the 'cast' to make the recommender even better!\n",
    "movies['tags'] = movies['overview'] + ' ' + movies['genres'] + ' ' + movies['keywords'] + ' ' + movies['cast']\n",
    "\n",
    "\n",
    "# --- Step 2: Perform the final text cleaning on the 'tags' column ---\n",
    "\n",
    "# Get the list of common English \"stop words\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_final_tags(text):\n",
    "    # Convert all text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove all punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove all numbers (optional, but good for this use case)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Split the text into words and remove the stop words\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    # Join the words back into a single string\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply this final cleaning function to our 'tags' column\n",
    "movies['cleaned_tags'] = movies['tags'].apply(clean_final_tags)\n",
    "\n",
    "\n",
    "# --- Step 3: Save the final, ready-to-use data ---\n",
    "\n",
    "# Select only the columns we need for the next stage of the project\n",
    "processed_df = movies[['movie_id', 'title', 'cleaned_tags']]\n",
    "\n",
    "# Save this fully processed data to a new CSV file\n",
    "processed_df.to_csv('processed_movies.csv', index=False)\n",
    "\n",
    "print(\"\\nData preprocessing is complete! The final cleaned data has been saved to 'processed_movies.csv'\")\n",
    "print(\"\\nHere's a sample of the final data:\")\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc5119b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "\u001b[K     |████████████████████████████████| 483 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.8.0-cp39-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 73.6 MB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting huggingface-hub>=0.20.0\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[K     |████████████████████████████████| 561 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.6.1)\n",
      "Collecting sympy>=1.13.3\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 55.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 18.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 19.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "\u001b[K     |████████████████████████████████| 432 kB 26.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.29)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "\u001b[K     |████████████████████████████████| 161 kB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.3-cp39-cp39-macosx_10_9_universal2.whl (207 kB)\n",
      "\u001b[K     |████████████████████████████████| 207 kB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests, pyyaml, hf-xet, fsspec, filelock, mpmath, MarkupSafe, huggingface-hub, tokenizers, sympy, safetensors, networkx, jinja2, transformers, torch, sentence-transformers\n",
      "Successfully installed MarkupSafe-3.0.2 certifi-2025.8.3 charset-normalizer-3.4.3 filelock-3.19.1 fsspec-2025.7.0 hf-xet-1.1.9 huggingface-hub-0.34.4 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 networkx-3.2.1 pyyaml-6.0.2 requests-2.32.5 safetensors-0.6.2 sentence-transformers-5.1.0 sympy-1.14.0 tokenizers-0.22.0 torch-2.8.0 transformers-4.56.0 urllib3-2.5.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityakumar/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/adityakumar/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the sentence transformer model...\n",
      "Generating vector embeddings... (This may take a few minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 151/151 [00:10<00:00, 13.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully generated and saved embeddings with shape: (4803, 384)\n",
      "The embeddings are saved in 'movie_embeddings.npy'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load the processed data you just created\n",
    "df = pd.read_csv('processed_movies.csv')\n",
    "\n",
    "# Handle any potential empty tags that might have resulted from cleaning\n",
    "df.dropna(subset=['cleaned_tags'], inplace=True)\n",
    "\n",
    "# 2. Load a powerful pre-trained model\n",
    "print(\"Loading the sentence transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# 3. Generate the embeddings for all movies\n",
    "print(\"Generating vector embeddings... (This may take a few minutes)\")\n",
    "embeddings = model.encode(df['cleaned_tags'].tolist(), show_progress_bar=True)\n",
    "\n",
    "\n",
    "# 4. Save the embeddings to a file\n",
    "np.save('movie_embeddings.npy', embeddings)\n",
    "\n",
    "print(f\"\\nSuccessfully generated and saved embeddings with shape: {embeddings.shape}\")\n",
    "print(\"The embeddings are saved in 'movie_embeddings.npy'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98cd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp39-cp39-macosx_14_0_arm64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<3.0,>=1.25.0 in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /Users/adityakumar/Library/Python/3.9/lib/python/site-packages (from faiss-cpu) (25.0)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully created and saved a FAISS index with 4803 movies.\n",
      "The index is saved in 'movie_index.faiss'\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu\n",
    "import numpy as np\n",
    "import faiss\n",
    "# 1. Load the embeddings you just created\n",
    "embeddings = np.load('movie_embeddings.npy')\n",
    "\n",
    "# 2. Get the dimension of the vectors (it should be 384 for our model)\n",
    "d = embeddings.shape[1]\n",
    "# 3. Create a FAISS index. IndexFlatL2 is a standard, accurate index.\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# 4. Add all of our movie embeddings to this index\n",
    "index.add(embeddings)\n",
    "\n",
    "# 5. Save the finished index to a file. This is our search engine!\n",
    "faiss.write_index(index, 'movie_index.faiss')\n",
    "\n",
    "print(f\"Successfully created and saved a FAISS index with {index.ntotal} movies.\")\n",
    "print(\"The index is saved in 'movie_index.faiss'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
